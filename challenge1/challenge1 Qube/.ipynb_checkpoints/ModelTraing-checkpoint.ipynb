{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling env:\n",
      "Qube-v0\n",
      "Observations Min:\n",
      "[[ -2.460008  -3.141512 -30.       -40.      ]]\n",
      "Observations Max:\n",
      "[[ 2.836503  3.141543 30.       40.      ]]\n",
      "Observations Mean:\n",
      "[[-0.003227  0.068037  0.03961   0.120729]]\n",
      "Observations Std:\n",
      "[[ 1.187617  2.292686  7.602608 10.660522]]\n",
      "Actions Min:\n",
      "[-14.998947]\n",
      "Actions Max:\n",
      "[14.997537]\n",
      "Actions Mean:\n",
      "[-0.013937]\n",
      "Actions Std:\n",
      "[8.636296]\n",
      "Rewards Min:\n",
      "-0.36041084\n",
      "Rewards Max:\n",
      "-0.0016006415\n",
      "Rewards Mean:\n",
      "-0.073501684\n",
      "Rewards Std:\n",
      "0.03843061\n",
      "(15469, 4)\n",
      "(15469, 1)\n",
      "(15469,)\n",
      "(15469, 4)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "import quanser_robots\n",
    "from quanser_robots import GentlyTerminating\n",
    "import time\n",
    "\n",
    "ENV_NAME= \"Qube-v0\"\n",
    "sampling_type=\"uniform\"\n",
    "env=GentlyTerminating(gym.make(ENV_NAME))\n",
    "\n",
    "sampling_type=\"uniform\"\n",
    "\n",
    "print(\"Sampling env:\")\n",
    "print(ENV_NAME)\n",
    "\n",
    "env=GentlyTerminating(gym.make(ENV_NAME))\n",
    "\n",
    "\n",
    "states=[]\n",
    "actions=[]\n",
    "rewards=[]\n",
    "next_states=[]\n",
    "\n",
    "num_samples=15000\n",
    "do_render=False\n",
    "\n",
    "arange=(env.action_space.high-env.action_space.low)[0]\n",
    "\n",
    "\n",
    "assert(env.action_space.low.shape==(1,))\n",
    "\n",
    "def random_action(mu,sigma):\n",
    "  if sampling_type==\"uniform\":\n",
    "    a=np.random.uniform(env.action_space.low[0],env.action_space.high[0],size=(1,))\n",
    "  elif sampling_type==\"discrete\":\n",
    "    a=np.random.choice([env.action_space.low[0],0,env.action_space.high[0]]).reshape(-1)\n",
    "  else:\n",
    "    a=np.random.normal(mu,sigma,size=(1,))\n",
    "    a=np.clip(a,env.action_space.low,env.action_space.high)\n",
    "  return a\n",
    "\n",
    "while len(states)<num_samples:\n",
    "  s=env.reset()\n",
    "  #sample initial action uniformly from the action space\n",
    "  mu=np.random.uniform(env.action_space.low[0],env.action_space.high[0])\n",
    "  #exponential sampling for sigma of our markov chain of random actions\n",
    "  sigma=np.exp(np.random.uniform(0,np.log(arange*2)))\n",
    "  done=False\n",
    "  step=0\n",
    "  while not done:\n",
    "    if do_render:\n",
    "        env.render()\n",
    "        time.sleep(0.016)\n",
    "    #do a step\n",
    "    \n",
    "    a=random_action(mu,sigma)\n",
    "    \n",
    "    s_,r,done,info=env.step(a)\n",
    "    #record data\n",
    "    if not done:\n",
    "      states.append(s)\n",
    "      actions.append(a)\n",
    "      rewards.append(r)\n",
    "      next_states.append(s_)\n",
    "    \n",
    "    \n",
    "    #update our Markov chain\n",
    "    mu=a[0]\n",
    "    #update current state\n",
    "    del s\n",
    "    s=s_\n",
    "    step+=1\n",
    "\n",
    "\n",
    "states=np.array(states)\n",
    "actions=np.array(actions)\n",
    "rewards=np.array(rewards)\n",
    "next_states=np.array(next_states)\n",
    "\n",
    "print(\"Observations Min:\")\n",
    "print(np.min(states,axis=0,keepdims=True))\n",
    "print(\"Observations Max:\")\n",
    "print(np.max(states,axis=0,keepdims=True))\n",
    "print(\"Observations Mean:\")\n",
    "print(np.mean(states,axis=0,keepdims=True))\n",
    "print(\"Observations Std:\")\n",
    "print(np.std(states,axis=0,keepdims=True))\n",
    "\n",
    "print(\"Actions Min:\")\n",
    "print(np.min(actions,axis=0))\n",
    "print(\"Actions Max:\")\n",
    "print(np.max(actions,axis=0))\n",
    "print(\"Actions Mean:\")\n",
    "print(np.mean(actions,axis=0))\n",
    "print(\"Actions Std:\")\n",
    "print(np.std(actions,axis=0))\n",
    "\n",
    "print(\"Rewards Min:\")\n",
    "print(np.min(rewards))\n",
    "print(\"Rewards Max:\")\n",
    "print(np.max(rewards))\n",
    "print(\"Rewards Mean:\")\n",
    "print(np.mean(rewards))\n",
    "print(\"Rewards Std:\")\n",
    "print(np.std(rewards))\n",
    "\n",
    "print(states.shape)\n",
    "print(actions.shape)\n",
    "print(rewards.shape)\n",
    "print(next_states.shape)\n",
    "\n",
    "np.save(f\"dynamics_samples/{ENV_NAME}_states.npy\",states)\n",
    "np.save(f\"dynamics_samples/{ENV_NAME}_actions.npy\",actions)\n",
    "np.save(f\"dynamics_samples/{ENV_NAME}_rewards.npy\",rewards)\n",
    "np.save(f\"dynamics_samples/{ENV_NAME}_next_states.npy\",next_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.utils.data as Data\n",
    "import torch.optim as optim\n",
    "# from collections import OrderedDict\n",
    "\n",
    "\n",
    "traing_size = int(num_samples*0.85)\n",
    "validation_size = num_samples - traing_size\n",
    "\n",
    "env_names={0:\"Levitation-v0\",1:\"CartpoleSwingShort-v0\",2:\"Qube-v0\",3:\"Pendulum-v2\"}\n",
    "ENV_NAME=env_names[2]\n",
    "\n",
    "states=np.load(f\"dynamics_samples/{ENV_NAME}_states.npy\")[:traing_size+validation_size]\n",
    "actions=np.load(f\"dynamics_samples/{ENV_NAME}_actions.npy\")[:traing_size+validation_size]\n",
    "rewards=np.load(f\"dynamics_samples/{ENV_NAME}_rewards.npy\")[:traing_size+validation_size].reshape(-1,1)\n",
    "next_states=np.load(f\"dynamics_samples/{ENV_NAME}_next_states.npy\")[:traing_size+validation_size]\n",
    "\n",
    "#----------------change the state---------------\n",
    "states = np.concatenate([\n",
    "    np.cos(states[:,0]).reshape(-1,1), np.sin(states[:,0]).reshape(-1,1), \n",
    "    np.cos(states[:,1]).reshape(-1,1), np.sin(states[:,1]).reshape(-1,1), \n",
    "    states[:,2].reshape(-1,1), states[:,3].reshape(-1,1)\n",
    "], axis = 1)\n",
    "\n",
    "next_states = np.concatenate([\n",
    "    np.cos(next_states[:,0]).reshape(-1,1), np.sin(next_states[:,0]).reshape(-1,1), \n",
    "    np.cos(next_states[:,1]).reshape(-1,1), np.sin(next_states[:,1]).reshape(-1,1), \n",
    "    next_states[:,2].reshape(-1,1), next_states[:,3].reshape(-1,1)\n",
    "], axis = 1)\n",
    "\n",
    "#-----------Normalization\n",
    "# Data Normalization\n",
    "# st_mean = states.mean(axis=0)\n",
    "# st_std = states.std(axis=0)\n",
    "# a_mean = actions.mean()\n",
    "# a_std = actions.std()\n",
    "# r_mean = rewards.mean()\n",
    "# r_std = states.std()\n",
    "\n",
    "#Actually it's range\n",
    "st_mean = states.min(axis=0)\n",
    "st_std = np.max(states - st_mean, axis=0)\n",
    "a_mean = actions.min(axis=0)\n",
    "a_std = np.max(actions-a_mean)\n",
    "r_mean = rewards.min(axis=0)# np.min(rewards)\n",
    "r_std = np.max(rewards-r_mean)\n",
    "\n",
    "states = (states-st_mean)/(st_std)\n",
    "next_states = (next_states-st_mean)/st_std\n",
    "rewards = (rewards-r_mean)/r_std\n",
    "actions = (actions-a_mean)/a_std\n",
    "\n",
    "np.savez(\"dynamics_models/Data_para.npz\", st_mean=st_mean, st_std=st_std, r_mean= r_mean, r_std=r_std,\n",
    "         a_mean=a_mean, a_std=a_std)\n",
    "\n",
    "random_index = np.random.permutation(traing_size+validation_size)\n",
    "traing_index = random_index[:traing_size]\n",
    "testing_index = random_index[traing_size:traing_size+validation_size]\n",
    "\n",
    "test_states, test_actions, test_rewards, test_next_states = \\\n",
    "     states[traing_index], actions[traing_index], \\\n",
    "    rewards[traing_index], next_states[traing_index]\n",
    "\n",
    "states, actions, rewards, next_states = \\\n",
    "    states[traing_index], actions[traing_index], rewards[traing_index], next_states[traing_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN Model\n",
      "Sequential(\n",
      "  (0): Linear(in_features=7, out_features=64, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=64, out_features=30, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n",
      "epoch:  1 traing_loss: 1.703752e-02 validation_loss: 3.944747e-03\n",
      "epoch:  2 traing_loss: 2.665099e-03 validation_loss: 2.619633e-03\n",
      "epoch:  3 traing_loss: 1.757165e-03 validation_loss: 1.362897e-03\n",
      "epoch:  4 traing_loss: 1.281352e-03 validation_loss: 1.025940e-03\n",
      "epoch:  5 traing_loss: 1.046962e-03 validation_loss: 1.614854e-03\n",
      "epoch:  6 traing_loss: 9.087934e-04 validation_loss: 6.618214e-04\n",
      "epoch:  7 traing_loss: 7.629573e-04 validation_loss: 6.885218e-04\n",
      "epoch:  8 traing_loss: 6.365075e-04 validation_loss: 5.721254e-04\n",
      "epoch:  9 traing_loss: 5.651330e-04 validation_loss: 4.133467e-04\n",
      "epoch:  10 traing_loss: 4.942822e-04 validation_loss: 6.936902e-04\n",
      "epoch:  11 traing_loss: 4.621497e-04 validation_loss: 3.502514e-04\n",
      "epoch:  12 traing_loss: 3.964572e-04 validation_loss: 3.678745e-04\n",
      "epoch:  13 traing_loss: 3.673255e-04 validation_loss: 3.847215e-04\n",
      "epoch:  14 traing_loss: 3.276326e-04 validation_loss: 2.883314e-04\n",
      "epoch:  15 traing_loss: 3.110043e-04 validation_loss: 2.632242e-04\n",
      "epoch:  16 traing_loss: 2.922516e-04 validation_loss: 2.571688e-04\n",
      "epoch:  17 traing_loss: 2.719527e-04 validation_loss: 5.184145e-04\n",
      "epoch:  18 traing_loss: 2.547784e-04 validation_loss: 2.419064e-04\n",
      "epoch:  19 traing_loss: 2.488947e-04 validation_loss: 2.978976e-04\n",
      "epoch:  20 traing_loss: 2.299010e-04 validation_loss: 2.520621e-04\n",
      "epoch:  21 traing_loss: 2.208930e-04 validation_loss: 2.299198e-04\n",
      "epoch:  22 traing_loss: 2.113361e-04 validation_loss: 1.993672e-04\n",
      "epoch:  23 traing_loss: 2.067816e-04 validation_loss: 1.920944e-04\n",
      "epoch:  24 traing_loss: 2.000287e-04 validation_loss: 1.912161e-04\n",
      "epoch:  25 traing_loss: 1.942751e-04 validation_loss: 2.453490e-04\n",
      "epoch:  26 traing_loss: 1.891611e-04 validation_loss: 1.817172e-04\n",
      "epoch:  27 traing_loss: 1.845222e-04 validation_loss: 1.885820e-04\n",
      "epoch:  28 traing_loss: 1.821656e-04 validation_loss: 1.758840e-04\n",
      "epoch:  29 traing_loss: 1.780234e-04 validation_loss: 1.742565e-04\n",
      "epoch:  30 traing_loss: 1.749126e-04 validation_loss: 2.929184e-04\n",
      "epoch:  31 traing_loss: 1.727613e-04 validation_loss: 1.716013e-04\n",
      "epoch:  32 traing_loss: 1.701460e-04 validation_loss: 1.666942e-04\n",
      "epoch:  33 traing_loss: 1.677514e-04 validation_loss: 1.721824e-04\n",
      "epoch:  34 traing_loss: 1.662670e-04 validation_loss: 1.678092e-04\n",
      "epoch:  35 traing_loss: 1.645188e-04 validation_loss: 1.694037e-04\n",
      "epoch:  36 traing_loss: 1.632134e-04 validation_loss: 1.610346e-04\n",
      "epoch:  37 traing_loss: 1.622351e-04 validation_loss: 1.678985e-04\n",
      "epoch:  38 traing_loss: 1.607250e-04 validation_loss: 1.601787e-04\n",
      "epoch:  39 traing_loss: 1.597286e-04 validation_loss: 1.634514e-04\n",
      "epoch:  40 traing_loss: 1.589339e-04 validation_loss: 1.646247e-04\n",
      "epoch:  41 traing_loss: 1.580987e-04 validation_loss: 1.566123e-04\n",
      "epoch:  42 traing_loss: 1.572911e-04 validation_loss: 1.572028e-04\n",
      "epoch:  43 traing_loss: 1.564679e-04 validation_loss: 1.555522e-04\n",
      "epoch:  44 traing_loss: 1.561842e-04 validation_loss: 1.548930e-04\n",
      "epoch:  45 traing_loss: 1.552944e-04 validation_loss: 1.607007e-04\n",
      "epoch:  46 traing_loss: 1.550882e-04 validation_loss: 1.542836e-04\n",
      "epoch:  47 traing_loss: 1.545813e-04 validation_loss: 1.552827e-04\n",
      "epoch:  48 traing_loss: 1.542471e-04 validation_loss: 1.569183e-04\n",
      "epoch:  49 traing_loss: 1.539929e-04 validation_loss: 1.539209e-04\n",
      "epoch:  50 traing_loss: 1.536588e-04 validation_loss: 1.530208e-04\n",
      "epoch:  51 traing_loss: 1.532939e-04 validation_loss: 1.527907e-04\n",
      "epoch:  52 traing_loss: 1.530348e-04 validation_loss: 1.527772e-04\n",
      "epoch:  53 traing_loss: 1.527898e-04 validation_loss: 1.523736e-04\n",
      "epoch:  54 traing_loss: 1.526177e-04 validation_loss: 1.522132e-04\n",
      "epoch:  55 traing_loss: 1.524856e-04 validation_loss: 1.520555e-04\n",
      "epoch:  56 traing_loss: 1.523275e-04 validation_loss: 1.522069e-04\n",
      "epoch:  57 traing_loss: 1.520572e-04 validation_loss: 1.523944e-04\n",
      "epoch:  58 traing_loss: 1.519581e-04 validation_loss: 1.520901e-04\n",
      "epoch:  59 traing_loss: 1.518784e-04 validation_loss: 1.517844e-04\n",
      "epoch:  60 traing_loss: 1.517482e-04 validation_loss: 1.515507e-04\n",
      "epoch:  61 traing_loss: 1.516369e-04 validation_loss: 1.515512e-04\n",
      "epoch:  62 traing_loss: 1.515734e-04 validation_loss: 1.513980e-04\n",
      "epoch:  63 traing_loss: 1.515036e-04 validation_loss: 1.516569e-04\n",
      "epoch:  64 traing_loss: 1.514042e-04 validation_loss: 1.512732e-04\n"
     ]
    }
   ],
   "source": [
    "# Train Reward Model\n",
    "\n",
    "epochs = 64\n",
    "batch_size = 64\n",
    "\n",
    "inputs, outputs = np.concatenate((states,actions), axis = 1), rewards\n",
    "inputs, outputs = torch.tensor(inputs), torch.tensor(outputs, dtype=torch.float64)\n",
    "test_inputs, test_outputs = np.concatenate((test_states,test_actions), axis = 1), test_rewards\n",
    "test_inputs, test_outputs = torch.tensor(test_inputs), torch.tensor(test_outputs, dtype=torch.float64)\n",
    "\n",
    "reward_model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(7, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 30),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(30, 1)\n",
    ").double()   # Attention the double here!!\n",
    "print(\"NN Model\")\n",
    "print(reward_model)\n",
    "\n",
    "dataset = Data.TensorDataset(inputs, outputs)\n",
    "loader = Data.DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    ") \n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = optim.RMSprop(reward_model.parameters(), lr=0.001, centered=True)\n",
    "lr_scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=(0.9))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0.0\n",
    "    lr_scheduler.step()\n",
    "    for step, (batch_x, batch_y) in enumerate(loader):        \n",
    "        batch_y_p = reward_model(batch_x)\n",
    "        loss = criterion(batch_y_p, batch_y)\n",
    "        \n",
    "        epoch_loss += batch_y.shape[0] * loss.item()\n",
    "        #Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()    \n",
    "        optimizer.step()\n",
    "    \n",
    "    vali_loss = criterion(reward_model(test_inputs), test_outputs).item()\n",
    "    print(\"epoch: \", epoch+1, \"traing_loss: %e\"%(epoch_loss/traing_size), \"validation_loss: %e\"%vali_loss) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN Model\n",
      "Sequential(\n",
      "  (0): Linear(in_features=7, out_features=64, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=64, out_features=30, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=30, out_features=6, bias=True)\n",
      ")\n",
      "epoch:  0 traing_loss: 1.287443e-01 validation_loss: 5.714211e-02\n",
      "epoch:  1 traing_loss: 4.594796e-02 validation_loss: 3.438751e-02\n",
      "epoch:  2 traing_loss: 2.415894e-02 validation_loss: 1.477940e-02\n",
      "epoch:  3 traing_loss: 1.007754e-02 validation_loss: 6.742672e-03\n",
      "epoch:  4 traing_loss: 5.263503e-03 validation_loss: 4.085829e-03\n",
      "epoch:  5 traing_loss: 3.392670e-03 validation_loss: 2.799833e-03\n",
      "epoch:  6 traing_loss: 2.402021e-03 validation_loss: 2.064345e-03\n",
      "epoch:  7 traing_loss: 1.794971e-03 validation_loss: 1.572998e-03\n",
      "epoch:  8 traing_loss: 1.392248e-03 validation_loss: 1.251383e-03\n",
      "epoch:  9 traing_loss: 1.138925e-03 validation_loss: 1.055131e-03\n",
      "epoch:  10 traing_loss: 9.829593e-04 validation_loss: 9.288090e-04\n",
      "epoch:  11 traing_loss: 8.931297e-04 validation_loss: 8.742223e-04\n",
      "epoch:  12 traing_loss: 8.404939e-04 validation_loss: 8.257088e-04\n",
      "epoch:  13 traing_loss: 8.071335e-04 validation_loss: 7.951593e-04\n",
      "epoch:  14 traing_loss: 7.846715e-04 validation_loss: 7.783092e-04\n",
      "epoch:  15 traing_loss: 7.684045e-04 validation_loss: 7.631817e-04\n",
      "epoch:  16 traing_loss: 7.560666e-04 validation_loss: 7.527118e-04\n",
      "epoch:  17 traing_loss: 7.466466e-04 validation_loss: 7.422284e-04\n",
      "epoch:  18 traing_loss: 7.393101e-04 validation_loss: 7.351539e-04\n",
      "epoch:  19 traing_loss: 7.331869e-04 validation_loss: 7.296407e-04\n",
      "epoch:  20 traing_loss: 7.281227e-04 validation_loss: 7.258129e-04\n",
      "epoch:  21 traing_loss: 7.237111e-04 validation_loss: 7.229059e-04\n",
      "epoch:  22 traing_loss: 7.201499e-04 validation_loss: 7.188263e-04\n",
      "epoch:  23 traing_loss: 7.170500e-04 validation_loss: 7.149195e-04\n",
      "epoch:  24 traing_loss: 7.144764e-04 validation_loss: 7.130160e-04\n",
      "epoch:  25 traing_loss: 7.121459e-04 validation_loss: 7.108012e-04\n",
      "epoch:  26 traing_loss: 7.101880e-04 validation_loss: 7.088101e-04\n",
      "epoch:  27 traing_loss: 7.083131e-04 validation_loss: 7.075179e-04\n",
      "epoch:  28 traing_loss: 7.068867e-04 validation_loss: 7.066106e-04\n",
      "epoch:  29 traing_loss: 7.055584e-04 validation_loss: 7.045688e-04\n",
      "epoch:  30 traing_loss: 7.043814e-04 validation_loss: 7.036689e-04\n",
      "epoch:  31 traing_loss: 7.032779e-04 validation_loss: 7.027518e-04\n",
      "epoch:  32 traing_loss: 7.024592e-04 validation_loss: 7.022441e-04\n",
      "epoch:  33 traing_loss: 7.016875e-04 validation_loss: 7.010667e-04\n",
      "epoch:  34 traing_loss: 7.009315e-04 validation_loss: 7.003363e-04\n",
      "epoch:  35 traing_loss: 7.003017e-04 validation_loss: 6.999440e-04\n",
      "epoch:  36 traing_loss: 6.997452e-04 validation_loss: 6.993630e-04\n",
      "epoch:  37 traing_loss: 6.992443e-04 validation_loss: 6.988731e-04\n",
      "epoch:  38 traing_loss: 6.988402e-04 validation_loss: 6.984497e-04\n",
      "epoch:  39 traing_loss: 6.984084e-04 validation_loss: 6.980657e-04\n",
      "epoch:  40 traing_loss: 6.980591e-04 validation_loss: 6.977256e-04\n",
      "epoch:  41 traing_loss: 6.977094e-04 validation_loss: 6.974646e-04\n",
      "epoch:  42 traing_loss: 6.974159e-04 validation_loss: 6.971698e-04\n",
      "epoch:  43 traing_loss: 6.971646e-04 validation_loss: 6.969509e-04\n",
      "epoch:  44 traing_loss: 6.969273e-04 validation_loss: 6.967677e-04\n",
      "epoch:  45 traing_loss: 6.967237e-04 validation_loss: 6.965546e-04\n",
      "epoch:  46 traing_loss: 6.965448e-04 validation_loss: 6.963538e-04\n",
      "epoch:  47 traing_loss: 6.963679e-04 validation_loss: 6.962082e-04\n",
      "epoch:  48 traing_loss: 6.962237e-04 validation_loss: 6.960692e-04\n",
      "epoch:  49 traing_loss: 6.960800e-04 validation_loss: 6.959467e-04\n",
      "epoch:  50 traing_loss: 6.959671e-04 validation_loss: 6.958402e-04\n",
      "epoch:  51 traing_loss: 6.958493e-04 validation_loss: 6.957466e-04\n",
      "epoch:  52 traing_loss: 6.957574e-04 validation_loss: 6.956575e-04\n",
      "epoch:  53 traing_loss: 6.956770e-04 validation_loss: 6.955758e-04\n",
      "epoch:  54 traing_loss: 6.955965e-04 validation_loss: 6.955062e-04\n",
      "epoch:  55 traing_loss: 6.955193e-04 validation_loss: 6.954437e-04\n",
      "epoch:  56 traing_loss: 6.954591e-04 validation_loss: 6.953863e-04\n",
      "epoch:  57 traing_loss: 6.954008e-04 validation_loss: 6.953353e-04\n",
      "epoch:  58 traing_loss: 6.953458e-04 validation_loss: 6.952875e-04\n",
      "epoch:  59 traing_loss: 6.952996e-04 validation_loss: 6.952456e-04\n"
     ]
    }
   ],
   "source": [
    "# Train State Model\n",
    "\n",
    "epochs = 60\n",
    "batch_size = 64\n",
    "\n",
    "inputs, outputs = np.concatenate((states,actions), axis = 1), next_states\n",
    "inputs, outputs = torch.tensor(inputs, dtype=torch.float64), torch.tensor(outputs, dtype=torch.float64)\n",
    "test_inputs, test_outputs = np.concatenate((test_states,test_actions), axis = 1), test_next_states\n",
    "test_inputs, test_outputs = torch.tensor(test_inputs, dtype=torch.float64), torch.tensor(test_outputs, dtype=torch.float64)\n",
    "\n",
    "state_model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(7, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 30),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(30, 6)\n",
    ").double()   # Attention the double here!!\n",
    "print(\"NN Model\")\n",
    "print(state_model)\n",
    "\n",
    "dataset = Data.TensorDataset(inputs, outputs)\n",
    "loader = Data.DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    ") \n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = optim.RMSprop(state_model.parameters(), lr=0.0001, centered=True)\n",
    "lr_scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=(0.9))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    lr_scheduler.step()\n",
    "    epoch_loss = 0.0\n",
    "    for step, (batch_x, batch_y) in enumerate(loader):        \n",
    "        batch_y_p = state_model(batch_x)\n",
    "        loss = criterion(batch_y_p, batch_y)\n",
    "        \n",
    "        epoch_loss += batch_y.shape[0] * loss.item()\n",
    "        #Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()    \n",
    "        optimizer.step()\n",
    "    \n",
    "    vali_loss = criterion(state_model(test_inputs), test_outputs).item()\n",
    "    print(\"epoch: \", epoch, \"traing_loss: %e\"%(epoch_loss/traing_size), \"validation_loss: %e\"%vali_loss)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_model.eval()\n",
    "state_model.eval()\n",
    "# Save the model\n",
    "torch.save(reward_model, f\"dynamics_models/Pendulum_rewards.pth\")\n",
    "torch.save(state_model, f\"dynamics_models/Pendulum_states.pth\")"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "186ddc33f26a4700aa7a24e8732d4bcc",
   "lastKernelId": "6b8ad8bf-703b-4a32-b0ed-d6aff806a0dc"
  },
  "kernelspec": {
   "display_name": "Challenge1",
   "language": "python",
   "name": "challenge1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
