{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling env:\n",
      "Pendulum-v2\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Observation space:\n",
      "Box(2,)\n",
      "Low:\n",
      "[-3.141593 -8.      ]\n",
      "High:\n",
      "[3.141593 8.      ]\n",
      "Action space:\n",
      "Box(1,)\n",
      "Low:\n",
      "[-2.]\n",
      "High:\n",
      "[2.]\n",
      "Action Range:\n",
      "4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhiyuan/anaconda3/envs/Challenge1/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations Min:\n",
      "[[-3.141579 -8.      ]]\n",
      "Observations Max:\n",
      "[[3.141583 8.      ]]\n",
      "Observations Mean:\n",
      "[[ 0.002675 -0.014893]]\n",
      "Observations Std:\n",
      "[[2.230247 3.484919]]\n",
      "Actions Min:\n",
      "[-1.999993]\n",
      "Actions Max:\n",
      "[1.999979]\n",
      "Actions Mean:\n",
      "[0.000722]\n",
      "Actions Std:\n",
      "[1.155866]\n",
      "Rewards Min:\n",
      "-16.26697384780663\n",
      "Rewards Max:\n",
      "-0.0010147630483370793\n",
      "Rewards Mean:\n",
      "-6.189834311790467\n",
      "Rewards Std:\n",
      "3.617299854074057\n",
      "(50148, 2)\n",
      "(50148, 1)\n",
      "(50148,)\n",
      "(50148, 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "import quanser_robots\n",
    "from quanser_robots import GentlyTerminating\n",
    "import time\n",
    "\n",
    "#MagLev, CartPoleSwingUp, FurutaPend\n",
    "#MagLev=Levitation-v0 ???\n",
    "#CartPoleSwingUp=CartpoleSwingShort-v0\n",
    "#FuturaPend=Qube-v0\n",
    "\n",
    "env_names={0:\"Levitation-v0\",1:\"CartpoleSwingShort-v0\",2:\"Qube-v0\",3:\"Pendulum-v2\"}\n",
    "\n",
    "\n",
    "ENV_NAME=env_names[3]\n",
    "sampling_type=\"uniform\"\n",
    "\n",
    "print(\"Sampling env:\")\n",
    "print(ENV_NAME)\n",
    "\n",
    "env=GentlyTerminating(gym.make(ENV_NAME))\n",
    "print(\"Observation space:\")\n",
    "print(env.observation_space)\n",
    "print(\"Low:\")\n",
    "print(env.observation_space.low)\n",
    "print(\"High:\")\n",
    "print(env.observation_space.high)\n",
    "print(\"Action space:\")\n",
    "print(env.action_space)\n",
    "print(\"Low:\")\n",
    "print(env.action_space.low)\n",
    "print(\"High:\")\n",
    "print(env.action_space.high)\n",
    "\n",
    "states=[]\n",
    "actions=[]\n",
    "rewards=[]\n",
    "next_states=[]\n",
    "\n",
    "num_samples=50000\n",
    "do_render=False\n",
    "\n",
    "arange=(env.action_space.high-env.action_space.low)[0]\n",
    "print(\"Action Range:\")\n",
    "print(arange)\n",
    "\n",
    "assert(env.action_space.low.shape==(1,))\n",
    "\n",
    "def random_action(mu,sigma):\n",
    "  if sampling_type==\"uniform\":\n",
    "    a=np.random.uniform(env.action_space.low[0],env.action_space.high[0],size=(1,))\n",
    "  elif sampling_type==\"discrete\":\n",
    "    a=np.random.choice([env.action_space.low[0],0,env.action_space.high[0]]).reshape(-1)\n",
    "  else:\n",
    "    a=np.random.normal(mu,sigma,size=(1,))\n",
    "    a=np.clip(a,env.action_space.low,env.action_space.high)\n",
    "  return a\n",
    "\n",
    "while len(states)<num_samples:\n",
    "  s=env.reset()\n",
    "  #sample initial action uniformly from the action space\n",
    "  mu=np.random.uniform(env.action_space.low[0],env.action_space.high[0])\n",
    "  #exponential sampling for sigma of our markov chain of random actions\n",
    "  sigma=np.exp(np.random.uniform(0,np.log(arange*4)))\n",
    "  done=False\n",
    "  step=0\n",
    "  while not done:\n",
    "    if do_render:\n",
    "        env.render()\n",
    "        time.sleep(0.016)\n",
    "    #do a step\n",
    "    if step>-1:\n",
    "        a=random_action(mu,sigma)\n",
    "    else:\n",
    "        #Bring Pendulum to bottom stand still.\n",
    "        a=np.clip(-1*s[-1:],env.action_space.low[0],env.action_space.high[0])\n",
    "    s_,r,done,info=env.step(a)\n",
    "    #record data\n",
    "    if not done:\n",
    "      states.append(s)\n",
    "      actions.append(a)\n",
    "      rewards.append(r)\n",
    "      next_states.append(s_)\n",
    "      \n",
    "    #update our Markov chain\n",
    "    mu=a[0]\n",
    "    #update current state\n",
    "    del s\n",
    "    s=s_\n",
    "    step+=1\n",
    "\n",
    "\n",
    "states=np.array(states)\n",
    "actions=np.array(actions)\n",
    "rewards=np.array(rewards)\n",
    "next_states=np.array(next_states)\n",
    "\n",
    "print(\"Observations Min:\")\n",
    "print(np.min(states,axis=0,keepdims=True))\n",
    "print(\"Observations Max:\")\n",
    "print(np.max(states,axis=0,keepdims=True))\n",
    "print(\"Observations Mean:\")\n",
    "print(np.mean(states,axis=0,keepdims=True))\n",
    "print(\"Observations Std:\")\n",
    "print(np.std(states,axis=0,keepdims=True))\n",
    "\n",
    "print(\"Actions Min:\")\n",
    "print(np.min(actions,axis=0))\n",
    "print(\"Actions Max:\")\n",
    "print(np.max(actions,axis=0))\n",
    "print(\"Actions Mean:\")\n",
    "print(np.mean(actions,axis=0))\n",
    "print(\"Actions Std:\")\n",
    "print(np.std(actions,axis=0))\n",
    "\n",
    "print(\"Rewards Min:\")\n",
    "print(np.min(rewards))\n",
    "print(\"Rewards Max:\")\n",
    "print(np.max(rewards))\n",
    "print(\"Rewards Mean:\")\n",
    "print(np.mean(rewards))\n",
    "print(\"Rewards Std:\")\n",
    "print(np.std(rewards))\n",
    "\n",
    "print(states.shape)\n",
    "print(actions.shape)\n",
    "print(rewards.shape)\n",
    "print(next_states.shape)\n",
    "\n",
    "np.save(f\"dynamics_samples/{ENV_NAME}_states.npy\",states)\n",
    "np.save(f\"dynamics_samples/{ENV_NAME}_actions.npy\",actions)\n",
    "np.save(f\"dynamics_samples/{ENV_NAME}_rewards.npy\",rewards)\n",
    "np.save(f\"dynamics_samples/{ENV_NAME}_next_states.npy\",next_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.utils.data as Data\n",
    "import torch.optim as optim\n",
    "# from collections import OrderedDict\n",
    "\n",
    "traing_size = 8000\n",
    "validation_size = 2000\n",
    "\n",
    "\n",
    "\n",
    "env_names={0:\"Levitation-v0\",1:\"CartpoleSwingShort-v0\",2:\"Qube-v0\",3:\"Pendulum-v2\"}\n",
    "ENV_NAME=env_names[3]\n",
    "\n",
    "states=np.load(f\"dynamics_samples/{ENV_NAME}_states.npy\")[:traing_size+validation_size]\n",
    "actions=np.load(f\"dynamics_samples/{ENV_NAME}_actions.npy\")[:traing_size+validation_size]\n",
    "rewards=np.load(f\"dynamics_samples/{ENV_NAME}_rewards.npy\")[:traing_size+validation_size].reshape(-1,1)\n",
    "next_states=np.load(f\"dynamics_samples/{ENV_NAME}_next_states.npy\")[:traing_size+validation_size]\n",
    "\n",
    "#----------------change the state---------------\n",
    "states = np.concatenate([\n",
    "    np.sin(states[:,0]).reshape(-1,1), np.cos(states[:,0]).reshape(-1,1), \n",
    "    states[:,1].reshape(-1,1)\n",
    "], axis = 1)\n",
    "\n",
    "next_states = np.concatenate([\n",
    "    np.sin(next_states[:,0]).reshape(-1,1), np.cos(next_states[:,0]).reshape(-1,1), \n",
    "    next_states[:,1].reshape(-1,1)\n",
    "], axis = 1)\n",
    "\n",
    "#-----------Normalization\n",
    "# Data Normalization\n",
    "# st_mean = states.mean(axis=0)\n",
    "# st_std = states.std(axis=0)\n",
    "# a_mean = actions.mean()\n",
    "# a_std = actions.std()\n",
    "# r_mean = rewards.mean()\n",
    "# r_std = states.std()\n",
    "\n",
    "#Actually it's range\n",
    "st_mean = states.min(axis=0)\n",
    "st_std = np.max(states - st_mean, axis=0)\n",
    "a_mean = -2\n",
    "a_std = 4\n",
    "r_mean = -20# np.min(rewards)\n",
    "r_std = 20 #np.max(rewards-r_mean)\n",
    "\n",
    "states = (states-st_mean)/(st_std)\n",
    "next_states = (next_states-st_mean)/st_std\n",
    "rewards = (rewards-r_mean)/r_std\n",
    "actions = (actions-a_mean)/a_std\n",
    "\n",
    "np.savez(\"dynamics_models/Data_para.npz\", st_mean=st_mean, st_std=st_std, r_mean= r_mean, r_std=r_std,\n",
    "         a_mean=a_mean, a_std=a_std)\n",
    "\n",
    "random_index = np.random.permutation(traing_size+validation_size)\n",
    "traing_index = random_index[:traing_size]\n",
    "testing_index = random_index[traing_size:traing_size+validation_size]\n",
    "\n",
    "test_states, test_actions, test_rewards, test_next_states = \\\n",
    "     states[traing_index], actions[traing_index], \\\n",
    "    rewards[traing_index], next_states[traing_index]\n",
    "\n",
    "states, actions, rewards, next_states = \\\n",
    "    states[traing_index], actions[traing_index], rewards[traing_index], next_states[traing_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN Model\n",
      "Sequential(\n",
      "  (0): Linear(in_features=4, out_features=64, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "epoch:  1 traing_loss: 1.355860e-02 validation_loss: 2.466834e-03\n",
      "epoch:  2 traing_loss: 1.380993e-03 validation_loss: 7.928850e-04\n",
      "epoch:  3 traing_loss: 6.317037e-04 validation_loss: 4.072150e-04\n",
      "epoch:  4 traing_loss: 4.238627e-04 validation_loss: 2.649629e-04\n",
      "epoch:  5 traing_loss: 3.544210e-04 validation_loss: 2.669881e-04\n",
      "epoch:  6 traing_loss: 3.164559e-04 validation_loss: 2.457882e-04\n",
      "epoch:  7 traing_loss: 2.773187e-04 validation_loss: 7.848275e-04\n",
      "epoch:  8 traing_loss: 2.584831e-04 validation_loss: 2.216999e-04\n",
      "epoch:  9 traing_loss: 2.242447e-04 validation_loss: 1.303898e-04\n",
      "epoch:  10 traing_loss: 2.140485e-04 validation_loss: 1.512313e-04\n",
      "epoch:  11 traing_loss: 1.948346e-04 validation_loss: 1.134746e-04\n",
      "epoch:  12 traing_loss: 1.816300e-04 validation_loss: 4.375205e-04\n",
      "epoch:  13 traing_loss: 1.690918e-04 validation_loss: 9.073941e-05\n",
      "epoch:  14 traing_loss: 1.700347e-04 validation_loss: 1.093338e-04\n",
      "epoch:  15 traing_loss: 1.455350e-04 validation_loss: 7.674352e-05\n",
      "epoch:  16 traing_loss: 1.525956e-04 validation_loss: 1.457504e-04\n",
      "epoch:  17 traing_loss: 1.347101e-04 validation_loss: 8.770496e-05\n",
      "epoch:  18 traing_loss: 1.355926e-04 validation_loss: 7.058894e-05\n",
      "epoch:  19 traing_loss: 1.203022e-04 validation_loss: 1.889823e-04\n",
      "epoch:  20 traing_loss: 1.270000e-04 validation_loss: 6.781323e-05\n",
      "epoch:  21 traing_loss: 1.185154e-04 validation_loss: 1.248209e-04\n",
      "epoch:  22 traing_loss: 1.173276e-04 validation_loss: 1.797153e-04\n",
      "epoch:  23 traing_loss: 1.082441e-04 validation_loss: 7.428150e-05\n",
      "epoch:  24 traing_loss: 1.048314e-04 validation_loss: 6.076088e-05\n",
      "epoch:  25 traing_loss: 1.045656e-04 validation_loss: 5.322146e-05\n",
      "epoch:  26 traing_loss: 9.764582e-05 validation_loss: 6.345756e-05\n",
      "epoch:  27 traing_loss: 9.717025e-05 validation_loss: 1.086056e-04\n",
      "epoch:  28 traing_loss: 9.327999e-05 validation_loss: 5.626760e-05\n",
      "epoch:  29 traing_loss: 9.318350e-05 validation_loss: 6.440512e-05\n",
      "epoch:  30 traing_loss: 8.298336e-05 validation_loss: 4.288404e-05\n",
      "epoch:  31 traing_loss: 8.801924e-05 validation_loss: 1.413942e-04\n",
      "epoch:  32 traing_loss: 8.262195e-05 validation_loss: 1.364188e-04\n",
      "epoch:  33 traing_loss: 7.737122e-05 validation_loss: 1.464142e-04\n",
      "epoch:  34 traing_loss: 8.032604e-05 validation_loss: 5.003293e-05\n",
      "epoch:  35 traing_loss: 7.122154e-05 validation_loss: 5.999881e-05\n",
      "epoch:  36 traing_loss: 7.544192e-05 validation_loss: 1.125518e-04\n",
      "epoch:  37 traing_loss: 7.020723e-05 validation_loss: 4.255489e-05\n",
      "epoch:  38 traing_loss: 6.720469e-05 validation_loss: 7.197027e-05\n",
      "epoch:  39 traing_loss: 6.891133e-05 validation_loss: 4.249336e-05\n",
      "epoch:  40 traing_loss: 6.772170e-05 validation_loss: 1.114297e-04\n"
     ]
    }
   ],
   "source": [
    "# Train Reward Model\n",
    "\n",
    "epochs = 40\n",
    "batch_size = 32\n",
    "\n",
    "inputs, outputs = np.concatenate((states,actions), axis = 1), rewards\n",
    "inputs, outputs = torch.tensor(inputs), torch.tensor(outputs)\n",
    "test_inputs, test_outputs = np.concatenate((test_states,test_actions), axis = 1), test_rewards\n",
    "test_inputs, test_outputs = torch.tensor(test_inputs), torch.tensor(test_outputs)\n",
    "\n",
    "reward_model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(4, 64),\n",
    "            torch.nn.ReLU(),\n",
    "#             torch.nn.Linear(64, 64),\n",
    "#             torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 1)\n",
    ").double()   # Attention the double here!!\n",
    "print(\"NN Model\")\n",
    "print(reward_model)\n",
    "\n",
    "dataset = Data.TensorDataset(inputs, outputs)\n",
    "loader = Data.DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    ") \n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = optim.RMSprop(reward_model.parameters(), lr=0.001, centered=True)\n",
    "lr_scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=(0.985))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0.0\n",
    "    lr_scheduler.step()\n",
    "    for step, (batch_x, batch_y) in enumerate(loader):        \n",
    "        batch_y_p = reward_model(batch_x)\n",
    "        loss = criterion(batch_y_p, batch_y)\n",
    "        \n",
    "        epoch_loss += batch_y.shape[0] * loss.item()\n",
    "        #Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()    \n",
    "        optimizer.step()\n",
    "    \n",
    "    vali_loss = criterion(reward_model(test_inputs), test_outputs).item()\n",
    "    print(\"epoch: \", epoch+1, \"traing_loss: %e\"%(epoch_loss/traing_size), \"validation_loss: %e\"%vali_loss) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN Model\n",
      "Sequential(\n",
      "  (0): Linear(in_features=4, out_features=64, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=64, out_features=3, bias=True)\n",
      ")\n",
      "epoch:  0 traing_loss: 1.303230e-02 validation_loss: 1.611085e-03\n",
      "epoch:  1 traing_loss: 1.211684e-03 validation_loss: 8.883705e-04\n",
      "epoch:  2 traing_loss: 7.338384e-04 validation_loss: 5.277916e-04\n",
      "epoch:  3 traing_loss: 4.679534e-04 validation_loss: 3.912612e-04\n",
      "epoch:  4 traing_loss: 3.567985e-04 validation_loss: 2.512761e-04\n",
      "epoch:  5 traing_loss: 2.921936e-04 validation_loss: 1.642805e-04\n",
      "epoch:  6 traing_loss: 2.381221e-04 validation_loss: 2.210274e-04\n",
      "epoch:  7 traing_loss: 2.029079e-04 validation_loss: 1.303139e-04\n",
      "epoch:  8 traing_loss: 2.019543e-04 validation_loss: 1.632487e-04\n",
      "epoch:  9 traing_loss: 1.716168e-04 validation_loss: 1.234586e-04\n",
      "epoch:  10 traing_loss: 1.749630e-04 validation_loss: 7.781458e-05\n",
      "epoch:  11 traing_loss: 1.594830e-04 validation_loss: 1.608269e-04\n",
      "epoch:  12 traing_loss: 1.477561e-04 validation_loss: 2.017613e-04\n",
      "epoch:  13 traing_loss: 1.426183e-04 validation_loss: 2.738103e-04\n",
      "epoch:  14 traing_loss: 1.287282e-04 validation_loss: 1.977559e-04\n",
      "epoch:  15 traing_loss: 1.216249e-04 validation_loss: 9.139782e-05\n",
      "epoch:  16 traing_loss: 1.304866e-04 validation_loss: 1.062155e-04\n",
      "epoch:  17 traing_loss: 1.118396e-04 validation_loss: 1.387277e-04\n",
      "epoch:  18 traing_loss: 1.194758e-04 validation_loss: 5.842987e-05\n",
      "epoch:  19 traing_loss: 1.138118e-04 validation_loss: 1.212106e-04\n",
      "epoch:  20 traing_loss: 9.993476e-05 validation_loss: 3.597387e-04\n",
      "epoch:  21 traing_loss: 1.084718e-04 validation_loss: 9.381930e-05\n",
      "epoch:  22 traing_loss: 1.016879e-04 validation_loss: 2.149434e-04\n",
      "epoch:  23 traing_loss: 9.206200e-05 validation_loss: 6.850159e-05\n",
      "epoch:  24 traing_loss: 9.367519e-05 validation_loss: 8.087135e-05\n",
      "epoch:  25 traing_loss: 8.387395e-05 validation_loss: 1.385379e-04\n",
      "epoch:  26 traing_loss: 9.141715e-05 validation_loss: 7.871408e-05\n",
      "epoch:  27 traing_loss: 8.256451e-05 validation_loss: 4.710764e-05\n",
      "epoch:  28 traing_loss: 8.569120e-05 validation_loss: 4.210000e-05\n",
      "epoch:  29 traing_loss: 7.865792e-05 validation_loss: 4.272271e-05\n",
      "epoch:  30 traing_loss: 7.931211e-05 validation_loss: 1.214766e-04\n",
      "epoch:  31 traing_loss: 7.256132e-05 validation_loss: 1.470012e-04\n",
      "epoch:  32 traing_loss: 7.858389e-05 validation_loss: 4.905355e-05\n",
      "epoch:  33 traing_loss: 7.059459e-05 validation_loss: 6.596328e-05\n",
      "epoch:  34 traing_loss: 7.051357e-05 validation_loss: 5.360004e-05\n",
      "epoch:  35 traing_loss: 6.487351e-05 validation_loss: 7.756229e-05\n",
      "epoch:  36 traing_loss: 6.717212e-05 validation_loss: 9.522953e-05\n",
      "epoch:  37 traing_loss: 6.588607e-05 validation_loss: 5.992075e-05\n",
      "epoch:  38 traing_loss: 6.479243e-05 validation_loss: 1.250553e-04\n",
      "epoch:  39 traing_loss: 6.036764e-05 validation_loss: 6.651584e-05\n",
      "epoch:  40 traing_loss: 6.253303e-05 validation_loss: 1.351579e-04\n",
      "epoch:  41 traing_loss: 5.686707e-05 validation_loss: 6.633363e-05\n",
      "epoch:  42 traing_loss: 5.721943e-05 validation_loss: 6.670035e-05\n",
      "epoch:  43 traing_loss: 5.923499e-05 validation_loss: 3.917760e-05\n",
      "epoch:  44 traing_loss: 4.993382e-05 validation_loss: 4.072096e-05\n",
      "epoch:  45 traing_loss: 5.469297e-05 validation_loss: 1.212475e-04\n",
      "epoch:  46 traing_loss: 5.456507e-05 validation_loss: 4.975188e-05\n",
      "epoch:  47 traing_loss: 4.934620e-05 validation_loss: 6.974366e-05\n",
      "epoch:  48 traing_loss: 5.144267e-05 validation_loss: 4.218528e-05\n",
      "epoch:  49 traing_loss: 4.887638e-05 validation_loss: 5.027761e-05\n",
      "epoch:  50 traing_loss: 4.664655e-05 validation_loss: 3.520269e-05\n",
      "epoch:  51 traing_loss: 4.870257e-05 validation_loss: 3.992827e-05\n",
      "epoch:  52 traing_loss: 4.549122e-05 validation_loss: 3.132606e-05\n",
      "epoch:  53 traing_loss: 4.526994e-05 validation_loss: 4.630628e-05\n",
      "epoch:  54 traing_loss: 4.288869e-05 validation_loss: 4.622902e-05\n",
      "epoch:  55 traing_loss: 4.251568e-05 validation_loss: 8.076318e-05\n",
      "epoch:  56 traing_loss: 4.336724e-05 validation_loss: 2.417935e-05\n",
      "epoch:  57 traing_loss: 4.269868e-05 validation_loss: 2.737992e-05\n",
      "epoch:  58 traing_loss: 4.045458e-05 validation_loss: 2.227978e-05\n",
      "epoch:  59 traing_loss: 4.025183e-05 validation_loss: 2.473950e-05\n"
     ]
    }
   ],
   "source": [
    "# Train State Model\n",
    "\n",
    "epochs = 60\n",
    "batch_size = 64\n",
    "\n",
    "inputs, outputs = np.concatenate((states,actions), axis = 1), next_states\n",
    "inputs, outputs = torch.tensor(inputs), torch.tensor(outputs)\n",
    "test_inputs, test_outputs = np.concatenate((test_states,test_actions), axis = 1), test_next_states\n",
    "test_inputs, test_outputs = torch.tensor(test_inputs), torch.tensor(test_outputs)\n",
    "\n",
    "state_model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(4, 64),\n",
    "            torch.nn.ReLU(),\n",
    "#             torch.nn.Linear(64, 64),\n",
    "#             torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 3)\n",
    ").double()   # Attention the double here!!\n",
    "print(\"NN Model\")\n",
    "print(state_model)\n",
    "\n",
    "dataset = Data.TensorDataset(inputs, outputs)\n",
    "loader = Data.DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    ") \n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = optim.RMSprop(state_model.parameters(), lr=0.001, centered=True)\n",
    "lr_scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=(0.985))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    lr_scheduler.step()\n",
    "    epoch_loss = 0.0\n",
    "    for step, (batch_x, batch_y) in enumerate(loader):        \n",
    "        batch_y_p = state_model(batch_x)\n",
    "        loss = criterion(batch_y_p, batch_y)\n",
    "        \n",
    "        epoch_loss += batch_y.shape[0] * loss.item()\n",
    "        #Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()    \n",
    "        optimizer.step()\n",
    "    \n",
    "    vali_loss = criterion(state_model(test_inputs), test_outputs).item()\n",
    "    print(\"epoch: \", epoch, \"traing_loss: %e\"%(epoch_loss/traing_size), \"validation_loss: %e\"%vali_loss)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_model.eval()\n",
    "state_model.eval()\n",
    "# Save the model\n",
    "torch.save(reward_model, f\"dynamics_models/Pendulum_rewards.pth\")\n",
    "torch.save(state_model, f\"dynamics_models/Pendulum_states.pth\")"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "770e619d094f4f398566e3321a7bf837",
   "lastKernelId": "c27d1df8-6d30-48ec-b1c5-833564e39128"
  },
  "kernelspec": {
   "display_name": "Challenge1",
   "language": "python",
   "name": "challenge1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
