{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state range:  [0.014   inf] [  0. -inf]\n",
      "action range:  [3.] [-3.]\n",
      "reward range (0.0, 0.002)\n",
      "mean_reward in this sampling  0.0019780649185180666\n",
      "the loss  15.435163058698208\n",
      "mean_reward in this sampling  0.0019788991928100584\n",
      "the loss  15.691746259923093\n",
      "mean_reward in this sampling  0.001977486991882324\n",
      "the loss  16.676628179679028\n",
      "mean_reward in this sampling  0.0019779144287109373\n",
      "the loss  15.887394420336932\n",
      "mean_reward in this sampling  0.0019783523559570313\n",
      "the loss  16.650239947484806\n",
      "mean_reward in this sampling  0.0019773521423339844\n",
      "the loss  15.209621546498965\n",
      "mean_reward in this sampling  0.0019770980834960938\n",
      "the loss  16.293607833911665\n",
      "mean_reward in this sampling  0.001978798294067383\n",
      "the loss  16.771787937548652\n",
      "mean_reward in this sampling  0.001978660202026367\n",
      "the loss  15.40987837749708\n",
      "mean_reward in this sampling  0.0019780969619750976\n",
      "the loss  16.069708995055407\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import quanser_robots\n",
    "from quanser_robots.cartpole import ctrl\n",
    "from quanser_robots.common import Logger, GentlyTerminating\n",
    "\n",
    "import numpy as np\n",
    "import scipy.linalg\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn, torch.optim\n",
    "import torch.utils.data as Data\n",
    "\n",
    "\n",
    "env = gym.make('Levitation-v0')\n",
    "env.seed(0)\n",
    "np.random.seed(2)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "print(\"state range: \", env.observation_space.high, env.observation_space.low)\n",
    "print(\"action range: \", env.action_space.high, env.action_space.low)\n",
    "print(\"reward range\", env.reward_range)\n",
    "\n",
    "class Policy:\n",
    "    def __init__(self):\n",
    "        net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(2, 32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(32, 32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(32, 2),\n",
    "        )   # TODO weigh initialization\n",
    "        \n",
    "        self.net = net\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "        self.optimizer = torch.optim.SGD(net.parameters(), lr=1e-3)\n",
    "        self.lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(self.optimizer, gamma=(0.985))\n",
    "    \n",
    "    def choose_action(self, s):\n",
    "        output = self.net(s).detach_().numpy()\n",
    "        mu = output[0,0]\n",
    "        sigma = output[0,1]\n",
    "        return np.random.normal(size = (1,)) * sigma + mu\n",
    "    \n",
    "    def train(self, states, actions, R_whole):  \n",
    "        states, actions, R_whole = torch.tensor(states, dtype=torch.float),\\\n",
    "            torch.tensor(actions, dtype=torch.float), torch.tensor(R_whole, dtype=torch.float)\n",
    "        dataset = Data.TensorDataset(states, actions, R_whole)\n",
    "        loader = Data.DataLoader(dataset=dataset, batch_size=64, shuffle=True, num_workers=2) \n",
    "        \n",
    "        epoch_loss = 0.\n",
    "        self.lr_scheduler.step()\n",
    "        for i, (s, a, R) in enumerate(loader):\n",
    "            para_distri = self.net(s)\n",
    "            mu, sigma = para_distri[:, [0]], para_distri[:, [1]]\n",
    "            log_pi = -torch.log(sigma) - 0.5 * ((a - mu)/sigma) ** 2  \n",
    "            loss = log_pi.view(1, -1) @ R.view(-1, 1)\n",
    "            \n",
    "            epoch_loss += s.shape[0] * loss.item()\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()    \n",
    "            self.optimizer.step()      \n",
    "        \n",
    "        print(\"the loss \", epoch_loss)\n",
    "        return\n",
    "        \n",
    "def sample_episode(env, action_policy, gamma):\n",
    "    lists_a, lists_s, lists_s_ne, lists_r = [], [], [], []\n",
    "    done = False\n",
    "    s = env.reset()\n",
    "    step_counter = 0\n",
    "    while not done:\n",
    "        a = action_policy(torch.tensor(s.reshape(1,-1), dtype = torch.float))\n",
    "        s_ne, r, done, _ = env.step(a)\n",
    "        lists_a.append(a), lists_s.append(s), lists_s_ne.append(s_ne), lists_r.append(r)\n",
    "        \n",
    "        step_counter+=1\n",
    "        s = s_ne\n",
    "    \n",
    "    assert step_counter == 5000 # not sure if the stepsize is fixed of 5000\n",
    "    \n",
    "    actions = np.array(lists_a)\n",
    "    states = np.array(lists_s)\n",
    "    rewards = np.array(lists_r)    \n",
    "    gamma_array = np.ones_like(rewards) * gamma\n",
    "    gamma_array = np.cumproduct(gamma_array)\n",
    "    R_fun = gamma_array * rewards\n",
    "    \n",
    "    print(\"mean_reward in this sampling \", np.sum(rewards)/step_counter)\n",
    "    return states, actions.reshape(-1,1), R_fun.reshape(-1,1), np.sum(rewards)/step_counter\n",
    "\n",
    "policy = Policy()\n",
    "for i in range(10):\n",
    "    states, actions, R_fun, mean_r = sample_episode(env, policy.choose_action, 0.99)\n",
    "    \n",
    "    policy.train(states, actions, R_fun)  "
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "8605e171175e467b95ff0a027cbf51c0",
   "lastKernelId": "b6577cad-763d-40c9-88ad-b7d4c7142cad"
  },
  "kernelspec": {
   "display_name": "RLLab",
   "language": "python",
   "name": "rllab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
